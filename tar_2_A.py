# -*- coding: utf-8 -*-
"""Copy of Better logistic with another layer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z4l2Vpi9oP1avjadYBa4QPTr0ZyLmYMa

Adapted from the homeworks with the simple logistic model from the Deep Learning Toar Sheni class last year with Dr. Elishai Ezra Tzur. We will now add one hidden layer with 3 nodes and see how that changes things.
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

"""### sigmoid(z)

Here we define our activation function; the sigmoid function 

s = $g(\theta^{T}x)$

$z = \theta^{T}x$

$g(z) = \frac{1}{1+e^{(-z)}}$

X := data set

$\theta$ := vector of weights

Compute the sigmoid of z (A scalar or numpy array of any size) returns s

Verify: sigmoid([0, 2]) = [ 0.5, 0.88079708]
"""


# sigmoid func
def sigmoid(z):
    X = np.exp(z)
    return X / (1 + X)


# tangent func
def tangent(z):
    return np.tanh(z)


# relu func
def relu(z):
    return np.maximum(z, 0)


# der

def sig_der(X):
    return sigmoid(X) * (1 - sigmoid(X))


def tanh_der(X):
    return 1 - X ** 2


def relu_der(X):
    return 1 * (X > 0)


# dictionary for funcs and derivatives to alternate between the different funcs
func_dict = {
    "sigmoid": [sigmoid, sig_der],
    "tangent": [tangent, tanh_der],
    "relu": [relu, relu_der]
}
"""Some examples of using this function. Notice that we can give it an array of values (not critical for us)"""

#print(sigmoid([0, 2]))
#print(sigmoid(2))
#print(sigmoid(np.array([4])))

"""### initialize_with_random: w, b
We don't use inialize with zero as zero values can be bad as we discussed in class.
Instead we inialize with random numbers.
"""

""" Initialize w and b for the both layers according to the number of the features and number of neurons in the layers.
W should be initialized randomly to small values (otherwise, values at the activation functions could be at the flat part).
"""


def initialize_parameters(n_x, n_h, n_y):
    return {
        "W1": np.random.randn(n_h, n_x) * 0.01,
        "b1": np.zeros([n_h, 1]),
        "W2": np.random.randn(n_y, n_h) * 0.01,
        "b2": np.zeros([n_y, 1]),
    }


# Toy example

#(initialize_parameters(4, 3, 1))

"""### forward propagate(X, thetas): 
retuns: A2 (the final value) and the cache of values
Implement the forward propagation
* parameters -- python dictionary containing your parameters (output of initialization function)
Note that thetas is now a cache of thetas (weights) 
* A2 -- The sigmoid output of the second activation
* cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"
"""


def forward_propagation(X, parameters, func):  # added to  parms the func used in this itration
    # Hidden Layer
    Z1 = parameters["W1"].dot(X) + parameters["b1"]
    A1 = func(Z1)  # activate the current func
    # Output Layer
    Z2 = parameters["W2"].dot(A1) + parameters["b2"]
    A2 = sigmoid(Z2)
    # print(A2)
    cache = {
        "Z1": Z1,
        "A1": A1,
        "Z2": Z2,
        "A2": A2
    }
    return A2, cache


"""Back_propagation calcuates the weight updates using the derivative of the different activation functions. These equations are similar to those in the lecture."""


def backward_propagation(parameters, cache, X, Y, der_func):  # added to  parms the der_func used in this itration
    m = X.shape[1]  # Number of samples
    # Output Layer
    dZ2 = cache["A2"] - Y  # for the sigmoid layer
    dW2 = (1 / m) * dZ2.dot(cache["A1"].T)
    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)
    # Hidden Layer
    dA1 = np.dot(parameters['W2'].T, dZ2)
    dZ1 = dA1 * der_func(cache['A1'])  # activate the current der_func
    dW1 = (1 / m) * np.dot(dZ1, X.T)
    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)
    return {"dW1": dW1, "dW2": dW2, "db1": db1, "db2": db2}


"""The cost function from the last example."""


def cost_calculation(A, Y):
    cost = np.mean(-(Y * np.log(A) + (1 - Y) * np.log(1 - (A))))
    return cost


"""Update the weights in the dictionary cache."""


def update_parameters(parameters, grads, learning_rate):
    return {
        "W1": parameters["W1"] - learning_rate * grads["dW1"],
        "W2": parameters["W2"] - learning_rate * grads["dW2"],
        "b1": parameters["b1"] - learning_rate * grads["db1"],
        "b2": parameters["b2"] - learning_rate * grads["db2"],
    }


"""### nn_model(X, Y, num_iterations, learning_rate): d
Builds the logistic regression model by calling the functions implemented above
* X_train -- training set represented by a numpy array of shape (number of features, m_train)
* Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)
* X_test -- test set represented by a numpy array of shape (number of features, m_test)
* Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)
* num_iterations -- hyperparameter representing the number of iterations to optimize the parameters
* learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()
* d -- dictionary containing information about the model. 
"""


def nn_model(X, Y, iterations, lr, n_h, der_func, func):  # added to  parms the der_func and func used in this itration
    n_x = X.shape[0]
    #(f" using {n_h} nodes in hidden layer:")
    n_y = 1
    parameters = initialize_parameters(n_x, n_h, n_y)
    #print("Network shape ", X.shape[0], n_h, n_y)
    for i in range(iterations):
        A2, cache = forward_propagation(X, parameters, func)
        cost = cost_calculation(A2, Y)
        grads = backward_propagation(parameters, cache, X, Y, der_func)
        parameters = update_parameters(parameters, grads, lr)
        costs.append(cost)
        # cost check
        # if i % 500 == 0:
        #     print(f"Cost after iteration {i}: {cost}")
    return parameters, costs


"""### predict(X, parameters): Y_prediction"""


def predict(X, parameters, func):  # added to  parms the used in this itration
    A2, cache = forward_propagation(X, parameters, func)
    return np.rint(A2)
    '''This round the values like:
    def predict(X, theta, threshold=0.5):
    if predict_probs(X, theta) >= threshold:
        return 1
        print(A)'''


def prediction_accuracy(y_pred, y_true):
    return np.mean(y_pred == y_true)


import pandas as pd
from sklearn.model_selection import train_test_split

url = 'https://github.com/rosenfa/nn/blob/master/pima-indians-diabetes.csv?raw=true'
# url = 'https://github.com/rosenfa/nn/blob/master/class2/spam.csv?raw=true'
df = pd.read_csv(url, header=0, error_bad_lines=False)
features = df.drop(['Outcome'], axis=1)
features = ((features - features.mean()) / features.std())
X = np.array(features)
Y = np.array(df['Outcome'])
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)
df

from sklearn.linear_model import LogisticRegression

sk_model = LogisticRegression()
sk_model.fit(X_train, Y_train)
accuracy = sk_model.score(X_test, Y_test)
print("accuracy = ", accuracy * 100, "%")
# print(Y_train)

X_train, X_test = X_train.T, X_test.T

num_iterations = [500, 1000, 1500, 2000]  # number of iterations
alpha = 0.5  # learning rate
costs = []

# 3 loops to alternate between different running options
for func in func_dict:  # func type loop

    dict_node_iter_train = {
        "1": [],
        "2": [],
        "3": [],
        "4": [],
        "5": []
    }

    dict_node_iter_test = {
        "1": [],
        "2": [],
        "3": [],
        "4": [],
        "5": []
    }

    print(f"\n using {func} function: ")
    for x_num_of_hidden_nodes in list(range(1, 6)):  # number of nodes loop
        for x_num_iterations in num_iterations:  # number of itartions loop
            #print(f"\n using {x_num_iterations} num of iterations: ")

            parameters, costs = nn_model(X_train, Y_train, x_num_iterations, alpha, x_num_of_hidden_nodes,
                                         func_dict[func][1], func_dict[func][
                                             0])  # choses from the dict of funcs the current func and derivative
            Y_train_predict = predict(X_train, parameters, func_dict[func][0])


            train_acc = prediction_accuracy(Y_train_predict, Y_train)
            Y_test_predict = predict(X_test, parameters, func_dict[func][0])
            test_acc = prediction_accuracy(Y_test_predict, Y_test)
            parameters["train_accuracy"] = train_acc
            parameters["test_accuracy"] = test_acc

            plt.plot(costs)
            costs = []
      #      plt.show()

            dict_node_iter_train[str(x_num_of_hidden_nodes)].append(str(train_acc))
            dict_node_iter_test[str(x_num_of_hidden_nodes)].append(str(test_acc))
            # print("Training acc : ", str(train_acc))
            #print("Testing acc : ", str(test_acc))
   # print("Training acc : \n",dict_node_iter_train)
    #print("Training acc : \n",dict_node_iter_test)

    print("Tarining acc")
    df_train = pd.DataFrame(
        [dict_node_iter_train['1'],
         dict_node_iter_train['2'],
         dict_node_iter_train['3'],
         dict_node_iter_train['4'],
         dict_node_iter_train['5']],
        index=[1,2,3,4,5],
        columns=num_iterations
    )
    print (df_train.to_string())

    print("Test acc")
    df_test = pd.DataFrame(
        [dict_node_iter_test['1'],
         dict_node_iter_test['2'],
         dict_node_iter_test['3'],
         dict_node_iter_test['4'],
         dict_node_iter_test['5']],
        index=[1, 2, 3, 4, 5],
        columns=num_iterations
    )

    print(df_test.to_string())